import cv2
import numpy as np
from paddleocr import PaddleOCR
import re
import time
from collections import Counter
import base64

# åˆå§‹åŒ– PaddleOCR
# https://www.paddleocr.ai/latest/version3.x/pipeline_usage/OCR.html#1-ocr
ocr_reader = PaddleOCR(
    use_textline_orientation=True,
    lang="en",
    text_detection_model_name="PP-OCRv5_mobile_det",
    text_recognition_model_name="PP-OCRv5_mobile_rec",
    text_detection_model_dir=r".\PP-OCRv5_mobile_det",
    text_recognition_model_dir=r".\PP-OCRv5_mobile_rec",
)

conf_threshold = 0.5
frame_count = 0
start_time = time.time()
last_ocr_time = 0
text_counter = Counter()
text_conf_dict = {}  # è¨˜éŒ„æ¯å€‹æ–‡å­—æœ€é«˜ confidenceå’Œåº§æ¨™
serial_number = 1
out_of_area = True  # ä¸€é–‹å§‹å‡è¨­ä¸åœ¨æª¢æ¸¬å€
defect_margin = 30  # é‚Šç•Œç·©è¡
last_roi_image = None  # ä¿å­˜æœ€å¾Œä¸€æ¬¡çš„ROIåœ–åƒ

# æ¨™æº–åŒ–å°ºå¯¸
STANDARD_WIDTH = 400
STANDARD_HEIGHT = 300


def order_points(pts):
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect

def four_point_transform(image, pts):
    rect = order_points(pts)
    (tl, tr, br, bl) = rect
    widthA = np.linalg.norm(br - bl)
    widthB = np.linalg.norm(tr - tl)
    maxWidth = int(max(widthA, widthB))
    heightA = np.linalg.norm(tr - br)
    heightB = np.linalg.norm(tl - bl)
    maxHeight = int(max(heightA, heightB))
    if maxWidth <= 0 or maxHeight <= 0:
        return None
    dst = np.array([
        [0, 0],
        [maxWidth - 1, 0],
        [maxWidth - 1, maxHeight - 1],
        [0, maxHeight - 1]], dtype="float32")
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
    return warped

def normalize_image_size(image, target_width=STANDARD_WIDTH, target_height=STANDARD_HEIGHT):
    """å°‡åœ–ç‰‡æ¨™æº–åŒ–åˆ°æŒ‡å®šå°ºå¯¸"""
    if image is None:
        return None
    return cv2.resize(image, (target_width, target_height))

def bbox_to_minmax(bbox_points):
    """å°‡PaddleOCRçš„bboxé»è½‰æ›ç‚º[x_min, y_min, x_max, y_max]æ ¼å¼"""
    x_coords = [point[0] for point in bbox_points]
    y_coords = [point[1] for point in bbox_points]
    return [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]

def normalize_bbox_coordinates(bbox, original_width, original_height, target_width=STANDARD_WIDTH, target_height=STANDARD_HEIGHT):
    """å°‡bboxåº§æ¨™å¾åŸå§‹å°ºå¯¸æ­£è¦åŒ–åˆ°ç›®æ¨™å°ºå¯¸"""
    x_min, y_min, x_max, y_max = bbox
    
    # è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹
    scale_x = target_width / original_width
    scale_y = target_height / original_height
    
    # æ­£è¦åŒ–åº§æ¨™
    normalized_bbox = [
        int(x_min * scale_x),
        int(y_min * scale_y),
        int(x_max * scale_x),
        int(y_max * scale_y)
    ]
    
    return normalized_bbox

def run_ocr_and_collect_text(warped_img, conf_threshold=0.5):                
    results = {}
    try:
        ocr_results = ocr_reader.predict(warped_img)
        #print(f"ğŸ” ocr_results: {ocr_results}")  # â† çœ‹çœ‹æœ‰æ²’æœ‰åµæ¸¬åˆ°ä»»ä½•æ±è¥¿
        for page in ocr_results:
            rec_texts = page.get('rec_texts', [])
            rec_scores = page.get('rec_scores', [])
            rec_polys = page.get('rec_polys', [])  # å››é»åº§æ¨™
            #print(f"ğŸ“¦ texts={rec_texts}, scores={rec_scores}, boxes={rec_polys}")
            for poly, text, score in zip(rec_polys, rec_texts, rec_scores):
                if score >= conf_threshold:
                    clean_text = re.sub(r'[^A-Za-z0-9]', '', text)
                    if clean_text:
                        text_counter[clean_text] += 1
                        text_conf_dict[clean_text] = max(text_conf_dict.get(clean_text, 0.0), float(score))
                        results[clean_text] = {
                            "score": float(score),
                            "bbox": bbox_to_minmax(poly)
                        }
    except Exception as e:
        print("OCR error on warped:", e)
    return results

def image_to_base64(image):
    """å°†OpenCVå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    try:
        # å°†å›¾åƒç¼–ç ä¸ºJPEGæ ¼å¼
        _, buffer = cv2.imencode('.jpg', image)
        # è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
        image_base64 = base64.b64encode(buffer).decode('utf-8')
        return image_base64
    except Exception as e:
        print(f"Error converting image to base64: {e}")
        return ""

def calculate_iou(reference_warped_img, current_warped_img, reference_text, current_text, threshold=0.9):
    """
    è¨ˆç®—å…©å€‹çŸ©å½¢çš„IoU (Intersection over Union)é‡ç–Šç‡
    å…ˆå°‡å…©å¼µåœ–ç‰‡éƒ½æ¨™æº–åŒ–åˆ°400x300ï¼Œç„¶å¾Œé‡æ–°OCRç²å–æ¨™æº–åŒ–å¾Œçš„åº§æ¨™é€²è¡Œæ¯”è¼ƒ
    
    åƒæ•¸:
    - reference_warped_img: æ¨™æº–ç­”æ¡ˆçš„warpedåœ–ç‰‡
    - current_warped_img: ç•¶å‰æª¢æ¸¬çš„warpedåœ–ç‰‡  
    - reference_text: æ¨™æº–ç­”æ¡ˆçš„æ–‡å­—
    - current_text: ç•¶å‰æª¢æ¸¬çš„æ–‡å­—
    - threshold: IoUé–¾å€¼
    """
    try:
        # 1. å°‡å…©å¼µåœ–ç‰‡éƒ½æ¨™æº–åŒ–åˆ°400x300
        ref_normalized = cv2.resize(reference_warped_img, (STANDARD_WIDTH, STANDARD_HEIGHT))
        curr_normalized = cv2.resize(current_warped_img, (STANDARD_WIDTH, STANDARD_HEIGHT))
        
        # 2. å°æ¨™æº–åŒ–å¾Œçš„åœ–ç‰‡é‡æ–°é€²è¡ŒOCR
        ref_ocr_results = ocr_reader.predict(ref_normalized)
        curr_ocr_results = ocr_reader.predict(curr_normalized)
        
        # 3. å¾æ¨™æº–ç­”æ¡ˆä¸­æ‰¾åˆ°å°æ‡‰æ–‡å­—çš„bbox
        ref_bbox = None
        for page in ref_ocr_results:
            rec_texts = page.get('rec_texts', [])
            rec_polys = page.get('rec_polys', [])
            for poly, text in zip(rec_polys, rec_texts):
                clean_text = re.sub(r'[^A-Za-z0-9]', '', text)
                if clean_text == reference_text:
                    ref_bbox = bbox_to_minmax(poly)
                    break
            if ref_bbox:
                break
                
        # 4. å¾ç•¶å‰æª¢æ¸¬ä¸­æ‰¾åˆ°å°æ‡‰æ–‡å­—çš„bbox
        curr_bbox = None
        for page in curr_ocr_results:
            rec_texts = page.get('rec_texts', [])
            rec_polys = page.get('rec_polys', [])
            for poly, text in zip(rec_polys, rec_texts):
                clean_text = re.sub(r'[^A-Za-z0-9]', '', text)
                if clean_text == current_text:
                    curr_bbox = bbox_to_minmax(poly)
                    break
            if curr_bbox:
                break
        
        if ref_bbox is None or curr_bbox is None:
            print(f"âŒ ç„¡æ³•æ‰¾åˆ°æ–‡å­—åº§æ¨™: ref_bbox={ref_bbox}, curr_bbox={curr_bbox}")
            return 0.0, False
        
        # 5. è¨ˆç®—IoU
        box1, box2 = ref_bbox, curr_bbox
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])  
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 < x1 or y2 < y1:
            print(f"ğŸ“¦ ç„¡é‡ç–Š: ref_bbox={ref_bbox}, curr_bbox={curr_bbox}")
            return 0.0, False
            
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        iou = intersection / union if union > 0 else 0.0
        is_aligned = iou >= threshold
        
        print(f"ğŸ“ æ¨™æº–åŒ–å¾Œåº§æ¨™æ¯”è¼ƒ({STANDARD_WIDTH}x{STANDARD_HEIGHT}):")
        print(f"   æ¨™æº–ç­”æ¡ˆ '{reference_text}': {ref_bbox}")
        print(f"   ç•¶å‰æª¢æ¸¬ '{current_text}': {curr_bbox}")
        print(f"   IoU={iou:.3f}, å°é½Š={'âœ…' if is_aligned else 'âŒ'}")
        
        return iou, is_aligned
        
    except Exception as e:
        print(f"âŒ calculate_iou error: {e}")
        return 0.0, False

# å…¨å±€è®Šæ•¸å­˜å„²æ¨™æº–ç­”æ¡ˆåœ–ç‰‡å’Œæ–‡å­—
reference_warped_img = None
reference_text = None

# ç•¶å‰æª¢æ¸¬æœƒè©±çš„ä½ç½®è¨˜éŒ„
current_session_position_checked = False
current_session_position_status = "clear"
current_session_warped_img = None

def load_reference_image(ref_path):
    """è¼‰å…¥æ¨™æº–ç­”æ¡ˆåœ–ç‰‡ä¸¦è¨˜éŒ„warpedåœ–ç‰‡å’Œæ–‡å­—"""
    global reference_warped_img, reference_text
    
    try:
        ref_image = cv2.imread(ref_path)
        if ref_image is None:
            print(f"âŒ Cannot load reference image: {ref_path}")
            return False

        # æ‰¾æ¨™æº–ç­”æ¡ˆçš„çŸ©å½¢ä¸¦åšé€è¦–è®Šæ›
        ref_gray = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(ref_gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        max_area = 0
        best_approx = None
        valid_rectangles = 0
        
        for i, cnt in enumerate(contours):
            peri = cv2.arcLength(cnt, True)
            approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
            area = cv2.contourArea(approx)
 
            if len(approx) == 4:
                valid_rectangles += 1
                if area > 5000:
                    if area > max_area:
                        max_area = area
                        best_approx = approx.reshape(4, 2).astype(np.float32)
  
        if best_approx is None:
            print("âŒ Cannot find rectangle in reference image")
            return False

        # é€è¦–è®Šæ›
        ref_warped = four_point_transform(ref_image, best_approx)
        if ref_warped is None:
            print("âŒ Perspective transform failed")
            return False

        print(f"ğŸ“ æ¨™æº–åœ–é€è¦–è®Šæ›å¾Œå°ºå¯¸: {ref_warped.shape[:2]}")
        
        # ä¿å­˜warpedåœ–ç‰‡
        reference_warped_img = ref_warped.copy()
        
        # OCRæ¨™æº–ç­”æ¡ˆç²å–æ–‡å­—
        ocr_texts = run_ocr_and_collect_text(ref_warped, conf_threshold)
        
        if len(ocr_texts) != 0:
            # ä¿å­˜ç¬¬ä¸€å€‹æª¢æ¸¬åˆ°çš„æ–‡å­—
            reference_text = list(ocr_texts.keys())[0]
            print(f"ğŸ“ æ¨™æº–ç­”æ¡ˆæ–‡å­—: '{reference_text}'")
            return True
        return False

    except Exception as e:
        print(f"âŒ Error loading reference image: {e}")
        import traceback
        traceback.print_exc()
        return False

# è¼‰å…¥æ¨™æº–ç­”æ¡ˆ
reference_image_path = r".\reference.jpg"  # è«‹ä¿®æ”¹ç‚ºä½ çš„æ¨™æº–ç­”æ¡ˆåœ–ç‰‡è·¯å¾‘
reference_loaded = load_reference_image(reference_image_path)
print(f"âœ… Reference image loaded: {reference_loaded}")
if reference_loaded:
    print(f"ğŸ“ Reference text: '{reference_text}'")

# å½±ç‰‡è®€å–
video_path = r".\sample.mp4"
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    print("âŒ Cannot open video")
    exit()


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1
    elapsed_time = time.time() - start_time
    fps = frame_count / elapsed_time if elapsed_time > 0 else 0
    height, width = frame.shape[:2]

    right_half = frame[:, width//2:].copy()
    gray = cv2.cvtColor(right_half, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    max_area = 0
    best_approx = None
    best_rect = None
    for cnt in contours:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        if len(approx) == 4:
            area = cv2.contourArea(approx)
            if area > 5000:
                x, y, w, h = cv2.boundingRect(approx)
                aspect_ratio = w / float(h) if h != 0 else 0
                if 0.3 < aspect_ratio < 3.0:
                    if area > max_area:
                        max_area = area
                        best_approx = approx.reshape(4, 2).astype(np.float32)
                        best_rect = (x, y, w, h)

    if best_rect is not None:
        x, y, w, h = best_rect
        rect_center_x = x + w / 2
        rect_center_y = y + h / 2

        defect_x_min = defect_margin
        defect_x_max = right_half.shape[1] - defect_margin
        defect_y_min = defect_margin
        defect_y_max = right_half.shape[0] - defect_margin

        inside_area = (
            defect_x_min <= rect_center_x <= defect_x_max and
            defect_y_min <= rect_center_y <= defect_y_max
        )

        if inside_area:
            if out_of_area:
                text_counter = Counter()
                text_conf_dict.clear()
                # é‡ç½®ç•¶å‰æœƒè©±çš„ä½ç½®æª¢æŸ¥ç‹€æ…‹
                current_session_position_checked = False
                current_session_position_status = "clear"
                current_session_warped_img = None
                print("ğŸ“„ Re-entered defect area â†’ reset counter and position status")
                out_of_area = False
        else:
            # ä»åœ¨ç•«é¢ä½†è¶…å‡º defect area ä¸é‡ç½®ï¼Œç­‰æ¶ˆå¤±å†è™•ç†
            pass

        warped = None
        if best_approx is not None:
            warped = four_point_transform(right_half, best_approx)
        else:
            warped = right_half[y:y+h, x:x+w].copy()

        current_ocr_texts = run_ocr_and_collect_text(warped, conf_threshold)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œä½ç½®æª¢æ¸¬ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æª¢æ¸¬åˆ°æ–‡å­—ä¸”æœ‰æ¨™æº–ç­”æ¡ˆæ™‚é€²è¡Œï¼‰
        if (not current_session_position_checked and 
            current_ocr_texts and 
            reference_warped_img is not None and 
            reference_text is not None):
            
            # å–å¾—ç•¶å‰æª¢æ¸¬åˆ°çš„ç¬¬ä¸€å€‹æ–‡å­—é€²è¡Œä½ç½®æª¢æŸ¥
            first_detected_text = list(current_ocr_texts.keys())[0]
            print(f"ğŸ¯ é¦–æ¬¡æª¢æ¸¬åˆ°æ–‡å­— '{first_detected_text}'ï¼Œé€²è¡Œä½ç½®æª¢æŸ¥...")
            
            iou_value, is_aligned = calculate_iou(
                reference_warped_img, 
                warped, 
                reference_text, 
                first_detected_text, 
                threshold=0.6
            )
            
            current_session_position_status = "clear" if is_aligned else "misaligned"
            current_session_position_checked = True
            current_session_warped_img = warped.copy()  # ä¿å­˜é€™ä¸€å¹€çš„åœ–ç‰‡
            
            print(f"âœ… ä½ç½®æª¢æŸ¥å®Œæˆ: IoU={iou_value:.3f}, Status={current_session_position_status}")
            print("ğŸ’¡ æœ¬æ¬¡æœƒè©±ä¸å†é‡è¤‡é€²è¡Œä½ç½®æª¢æŸ¥")

        if warped is not None:
            roi_display = warped.copy()
            if current_ocr_texts:
                
                text_line = " | ".join(current_ocr_texts.keys())  # ç”¨ key ä¸²èµ·ä¾†é¡¯ç¤º
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.8
                font_thickness = 2
                (text_width, text_height), baseline = cv2.getTextSize(text_line, font, font_scale, font_thickness)
                padding = 10
                cv2.rectangle(roi_display, (0, 0), (roi_display.shape[1], text_height + padding * 2), (255, 255, 255), -1)
                cv2.rectangle(roi_display, (0, 0), (roi_display.shape[1], text_height + padding * 2), (0, 0, 0), 2)
                cv2.putText(roi_display, text_line, (padding, text_height + padding),
                            font, font_scale, (0, 0, 0), font_thickness)
            
            # ä¿å­˜åŒ…å«OCRæ–‡æœ¬å åŠ çš„æ¨™æº–åŒ–ROIåœ–åƒ
            last_roi_image = roi_display.copy()
            cv2.imshow("ROI + OCR Compare", roi_display)

        if best_approx is not None:
            pts_int = best_approx.astype(int)
            pts_int[:, 0] += width // 2
            cv2.polylines(frame, [pts_int], True, (255, 0, 0), 2)
        else:
            cv2.rectangle(frame, (x + width//2, y), (x + width//2 + w, y + h), (255, 0, 0), 2)

        top3 = text_counter.most_common(3)
        y0 = 30
        for i, (word, count) in enumerate(top3):
            y = y0 + i * 30
            cv2.putText(frame, f"{word}: {count}", (10, y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    else:
        # æ²’æœ‰æ‰¾åˆ°çŸ©å½¢ â†’ è¦–ç‚ºé›¢é–‹
        if not out_of_area:
            # åœ¨é›¢é–‹å‰è¼¸å‡º JSON
            if text_counter:
                top1, count = text_counter.most_common(1)[0]
                confidence = text_conf_dict.get(top1, 0.0)
                
                # ä½¿ç”¨å·²ç¶“æª¢æŸ¥éçš„ä½ç½®ç‹€æ…‹
                position_status = current_session_position_status
                print(f"ğŸ“‹ ä½¿ç”¨å·²è¨˜éŒ„çš„ä½ç½®ç‹€æ…‹: {position_status}")
                
                # å°‡æœ€å¾Œçš„ROIåœ–åƒè½‰æ›ç‚ºbase64ï¼ˆå„ªå…ˆä½¿ç”¨ä½ç½®æª¢æŸ¥æ™‚ä¿å­˜çš„åœ–ç‰‡ï¼‰
                image_base64 = ""
                roi_image_to_save = current_session_warped_img if current_session_warped_img is not None else last_roi_image
                if roi_image_to_save is not None:
                    image_base64 = image_to_base64(roi_image_to_save)

                result_json = {
                    serial_number: {
                        "value": top1,
                        "confidence": round(confidence, 3),
                        "status": position_status,
                        #"bbox": current_bbox,  # æ·»åŠ åº§æ¨™è³‡è¨Š
                        "image_base64": image_base64
                    }
                }
                print("ğŸ“¤ JSON result:", result_json)
                serial_number += 1

            print("â¬…ï¸ Lost detection â†’ mark as out of area")
            out_of_area = True

    # ç•«æª¢æ¸¬å€æ¡†ç·š
    cv2.line(frame, (width//2, 0), (width//2, height), (0, 255, 255), 2)

    cv2.putText(frame, f"Frame: {frame_count}", (10, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, 150),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

    cv2.imshow("Frame", frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()