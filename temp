import cv2
import numpy as np
import easyocr
import re
import time
from collections import Counter
import base64

# åˆå§‹åŒ– EasyOCR
# æ”¯æ´è‹±æ–‡ï¼Œå¯ä»¥æ ¹æ“šéœ€è¦æ·»åŠ å…¶ä»–èªè¨€ï¼Œä¾‹å¦‚ ['en', 'ch_sim'] æ”¯æ´ä¸­è‹±æ–‡
ocr_reader = easyocr.Reader(['en'], gpu=True)  # è¨­ç½®gpu=Falseå¦‚æœæ²’æœ‰GPU

conf_threshold = 0.5
frame_count = 0
start_time = time.time()
last_ocr_time = 0
text_counter = Counter()
text_conf_dict = {}  # è¨˜éŒ„æ¯å€‹æ–‡å­—æœ€é«˜ confidenceå’Œåº§æ¨™
serial_number = 1
out_of_area = True  # ä¸€é–‹å§‹å‡è¨­ä¸åœ¨æª¢æ¸¬å€
defect_margin = 30  # é‚Šç•Œç·©è¡
last_roi_image = None  # ä¿å­˜æœ€å¾Œä¸€æ¬¡çš„ROIåœ–åƒ
OCR_INTERVAL = 1 # æ¯å¹¾åµåšä¸€æ¬¡OCR
# æ¨™æº–åŒ–å°ºå¯¸
STANDARD_WIDTH = 320 #400
STANDARD_HEIGHT = 240 #300

# ä¸­é–“ç·šä½ç½®è¨­å®šï¼ˆå¯èª¿æ•´ï¼‰
# 0.5 = ç•«é¢æ­£ä¸­å¤®, 0.6 = åå³, 0.4 = åå·¦
LINE_POSITION_RATIO = 0.4  # èª¿æ•´é€™å€‹å€¼ä¾†æ”¹è®Šç·šçš„ä½ç½®


def order_points(pts):
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect

def four_point_transform(image, pts):
    rect = order_points(pts)
    (tl, tr, br, bl) = rect
    widthA = np.linalg.norm(br - bl)
    widthB = np.linalg.norm(tr - tl)
    maxWidth = int(max(widthA, widthB))
    heightA = np.linalg.norm(tr - br)
    heightB = np.linalg.norm(tl - bl)
    maxHeight = int(max(heightA, heightB))
    if maxWidth <= 0 or maxHeight <= 0:
        return None
    dst = np.array([
        [0, 0],
        [maxWidth - 1, 0],
        [maxWidth - 1, maxHeight - 1],
        [0, maxHeight - 1]], dtype="float32")
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
    return warped

def normalize_image_size(image, target_width=STANDARD_WIDTH, target_height=STANDARD_HEIGHT):
    """å°‡åœ–ç‰‡æ¨™æº–åŒ–åˆ°æŒ‡å®šå°ºå¯¸"""
    if image is None:
        return None
    return cv2.resize(image, (target_width, target_height))

def easyocr_bbox_to_minmax(bbox_points):
    """å°‡EasyOCRçš„bboxé»è½‰æ›ç‚º[x_min, y_min, x_max, y_max]æ ¼å¼"""
    # EasyOCRè¿”å›çš„bboxæ ¼å¼æ˜¯ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
    x_coords = [point[0] for point in bbox_points]
    y_coords = [point[1] for point in bbox_points]
    return [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]

def normalize_bbox_coordinates(bbox, original_width, original_height, target_width=STANDARD_WIDTH, target_height=STANDARD_HEIGHT):
    """å°‡bboxåº§æ¨™å¾åŸå§‹å°ºå¯¸æ­£è¦åŒ–åˆ°ç›®æ¨™å°ºå¯¸"""
    x_min, y_min, x_max, y_max = bbox
    
    # è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹
    scale_x = target_width / original_width
    scale_y = target_height / original_height
    
    # æ­£è¦åŒ–åº§æ¨™
    normalized_bbox = [
        int(x_min * scale_x),
        int(y_min * scale_y),
        int(x_max * scale_x),
        int(y_max * scale_y)
    ]
    
    return normalized_bbox

def run_ocr_and_collect_text(warped_img, conf_threshold=0.5):                
    results = {}
    try:
        # EasyOCRçš„readtextæ–¹æ³•è¿”å› (bbox, text, confidence)
        ocr_results = ocr_reader.readtext(warped_img)
        for (bbox, text, confidence) in ocr_results:
            if confidence >= conf_threshold:
                print(f"ğŸ” åŸå§‹OCRæ–‡å­—: '{text}' (confidence: {confidence:.3f})")
                # ä¿®æ”¹ï¼šå°‡éå­—æ¯æ•¸å­—å­—ç¬¦æ›¿æ›ç‚ºç©ºç™½ï¼Œç„¶å¾Œç§»é™¤ç©ºç™½
                clean_text = re.sub(r'[^A-Za-z0-9]', '', text.replace(' ', ''))
                print(f"âœ¨ æ¸…ç†å¾Œæ–‡å­—: '{clean_text}'")
                if clean_text:
                    text_counter[clean_text] += 1
                    text_conf_dict[clean_text] = max(text_conf_dict.get(clean_text, 0.0), float(confidence))
                    results[clean_text] = {
                        "score": float(confidence),
                        "bbox": easyocr_bbox_to_minmax(bbox)
                    }
    except Exception as e:
        print("OCR error on warped:", e)
    return results

def image_to_base64(image):
    """å°†OpenCVå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    try:
        # å°†å›¾åƒç¼–ç ä¸ºJPEGæ ¼å¼
        _, buffer = cv2.imencode('.jpg', image)
        # è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
        image_base64 = base64.b64encode(buffer).decode('utf-8')
        return image_base64
    except Exception as e:
        print(f"Error converting image to base64: {e}")
        return ""

def calculate_iou(current_warped_img, current_text, threshold=0.9):
    """
    è¨ˆç®—ç•¶å‰åœ–ç‰‡èˆ‡æ¨™æº–ç­”æ¡ˆçš„ IoU
    - current_warped_img: ç•¶å‰ warped ROI
    - current_text: ç•¶å‰æª¢æ¸¬åˆ°çš„æ–‡å­—
    - threshold: IoU é–¾å€¼
    """
    global reference_warped_img, reference_text, reference_bbox
    try:
        if reference_warped_img is None or reference_bbox is None:
            print("âŒ Reference å°šæœªæ­£ç¢ºè¼‰å…¥ï¼Œç„¡æ³•è¨ˆç®— IoU")
            return 0.0, False

        # åªå° current warped åš OCR
        curr_normalized = cv2.resize(current_warped_img, (STANDARD_WIDTH, STANDARD_HEIGHT))
        curr_ocr_results = ocr_reader.readtext(curr_normalized)

        curr_bbox = None
        for (bbox, text, confidence) in curr_ocr_results:
            clean_text = re.sub(r'[^A-Za-z0-9]', '', text.replace(' ', ''))
            if clean_text == current_text:
                curr_bbox = easyocr_bbox_to_minmax(bbox)
                break

        if curr_bbox is None:
            print(f"âŒ ç•¶å‰åœ–ç‰‡ç„¡æ³•æ‰¾åˆ°æ–‡å­— '{current_text}' çš„åº§æ¨™")
            return 0.0, False

        # è¨ˆç®— IoU
        box1, box2 = reference_bbox, curr_bbox
        x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])
        x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])

        if x2 < x1 or y2 < y1:
            print(f"ğŸ“¦ ç„¡é‡ç–Š: ref={box1}, curr={box2}")
            return 0.0, False

        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection

        iou = intersection / union if union > 0 else 0.0
        is_aligned = iou >= threshold

        print(f"ğŸ“ IoU={iou:.3f}, å°é½Š={'âœ…' if is_aligned else 'âŒ'}")
        return iou, is_aligned

    except Exception as e:
        print(f"âŒ calculate_iou error: {e}")
        return 0.0, False

# å…¨å±€è®Šæ•¸å­˜å„²æ¨™æº–ç­”æ¡ˆåœ–ç‰‡å’Œæ–‡å­—
reference_warped_img = None
reference_text = None
reference_bbox = None  # <--- ç´€éŒ„æ¨™æº–ç­”æ¡ˆæ–‡å­—çš„bbox

# ç•¶å‰æª¢æ¸¬æœƒè©±çš„ä½ç½®è¨˜éŒ„
current_session_position_checked = False
current_session_position_status = "clear"
current_session_warped_img = None

# è¿½è¹¤çŸ©å½¢æ˜¯å¦è·¨è¶Šä¸­é–“ç·š
rect_crossed_line = False
last_rect_x = None  # è¨˜éŒ„ä¸Šä¸€å¹€çŸ©å½¢çš„xåº§æ¨™

def load_reference_image(ref_path):
    """è¼‰å…¥æ¨™æº–ç­”æ¡ˆåœ–ç‰‡ä¸¦è¨˜éŒ„ warped åœ–ç‰‡ã€æ–‡å­—å’Œ bbox"""
    global reference_warped_img, reference_text, reference_bbox

    try:
        ref_image = cv2.imread(ref_path)
        if ref_image is None:
            print(f"âŒ Cannot load reference image: {ref_path}")
            return False

        # æ‰¾çŸ©å½¢è¼ªå»“
        ref_gray = cv2.cvtColor(ref_image, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(ref_gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        max_area = 0
        best_approx = None
        for cnt in contours:
            peri = cv2.arcLength(cnt, True)
            approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
            area = cv2.contourArea(approx)

            if len(approx) == 4 and area > 5000:
                if area > max_area:
                    max_area = area
                    best_approx = approx.reshape(4, 2).astype(np.float32)

        if best_approx is None:
            print("âŒ Cannot find rectangle in reference image")
            return False

        # é€è¦–è®Šæ›
        ref_warped = four_point_transform(ref_image, best_approx)
        if ref_warped is None:
            print("âŒ Perspective transform failed")
            return False

        print(f"ğŸ“ æ¨™æº–åœ–é€è¦–è®Šæ›å¾Œå°ºå¯¸: {ref_warped.shape[:2]}")

        # ä¿å­˜ warped åœ–ç‰‡
        reference_warped_img = ref_warped.copy()

        # OCR ä¸€æ¬¡ï¼Œå–ç¬¬ä¸€å€‹æ–‡å­— + bbox
        ocr_texts = run_ocr_and_collect_text(ref_warped, conf_threshold)
        if len(ocr_texts) > 0:
            reference_text = list(ocr_texts.keys())[0]
            reference_bbox = list(ocr_texts.values())[0]["bbox"]  # <--- å„²å­˜æ¨™æº– bbox
            print(f"ğŸ“ æ¨™æº–ç­”æ¡ˆæ–‡å­—: '{reference_text}', bbox={reference_bbox}")
            return True
        else:
            print("âŒ Reference image OCR æ²’æœ‰æ‰¾åˆ°æ–‡å­—")
            return False

    except Exception as e:
        print(f"âŒ Error loading reference image: {e}")
        import traceback
        traceback.print_exc()
        return False
        
# è¼‰å…¥æ¨™æº–ç­”æ¡ˆ
reference_image_path = r"./reference3.jpg"  # è«‹ä¿®æ”¹ç‚ºä½ çš„æ¨™æº–ç­”æ¡ˆåœ–ç‰‡è·¯å¾‘
reference_loaded = load_reference_image(reference_image_path)
print(f"âœ… Reference image loaded: {reference_loaded}")
if reference_loaded:
    print(f"ğŸ“ Reference text: '{reference_text}'")

# å½±ç‰‡è®€å–
video_path = r"./sample3.mp4"
cap = cv2.VideoCapture("sample3.mp4")
if not cap.isOpened():
    print("âŒ Cannot open video")
    exit()


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1
    elapsed_time = time.time() - start_time
    fps = frame_count / elapsed_time if elapsed_time > 0 else 0
    height, width = frame.shape[:2]
    line_x = int(width * LINE_POSITION_RATIO)  # è¨ˆç®—ç·šçš„xåº§æ¨™

    #â­ findContours å‰å…ˆç¸®å°ç•«é¢
    right_half = frame[:, line_x:].copy()
    small = cv2.resize(right_half, (right_half.shape[1]//2, right_half.shape[0]//2))
    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (3, 3), 0)
    edges = cv2.Canny(blur, 100, 200)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    max_area = 0
    best_rect = None
    best_approx = None
    for cnt in contours:
        cnt = cnt * 2  # â­ åº§æ¨™æ”¾å¤§å›ä¾†
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        if len(approx) == 4:
            area = cv2.contourArea(approx)
            if area > 5000:
                x, y, w, h = cv2.boundingRect(approx)
                aspect_ratio = w / float(h) if h != 0 else 0
                if 0.3 < aspect_ratio < 3.0 and area > max_area:
                    max_area = area
                    best_approx = approx.reshape(4, 2).astype(np.float32)
                    best_rect = (x, y, w, h)

    if best_rect is not None:
        x, y, w, h = best_rect
        rect_center_x = x + w / 2
        rect_center_y = y + h / 2

        defect_x_min = defect_margin
        defect_x_max = right_half.shape[1] - defect_margin
        defect_y_min = defect_margin
        defect_y_max = right_half.shape[0] - defect_margin

        inside_area = (
            defect_x_min <= rect_center_x <= defect_x_max and
            defect_y_min <= rect_center_y <= defect_y_max
        )

        # æª¢æŸ¥çŸ©å½¢æ˜¯å¦è·¨è¶Šä¸­é–“ç·šï¼ˆå¾å³å´é€²å…¥å·¦å´ï¼‰
        rect_center_x_in_full_frame = x + line_x + w/2  # çŸ©å½¢ä¸­å¿ƒåœ¨å®Œæ•´ç•«é¢ä¸­çš„xåº§æ¨™
        
        # æª¢æ¸¬è·¨è¶Šä¸­é–“ç·šçš„é‚è¼¯
        if last_rect_x is not None:
            # å¦‚æœçŸ©å½¢å¾å³å´ï¼ˆx > line_xï¼‰ç§»å‹•åˆ°å·¦å´ï¼ˆx <= line_xï¼‰ï¼Œè¡¨ç¤ºè·¨è¶Šäº†ä¸­é–“ç·š
            if last_rect_x > line_x and rect_center_x_in_full_frame <= line_x:
                rect_crossed_line = True
                text_counter = Counter()
                text_conf_dict.clear()
                # é‡ç½®ç•¶å‰æœƒè©±çš„ä½ç½®æª¢æŸ¥ç‹€æ…‹
                current_session_position_checked = False
                current_session_position_status = "clear"
                current_session_warped_img = None
                #print("ğŸš€ çŸ©å½¢è·¨è¶Šä¸­é–“ç·š â†’ é‡ç½®è¨ˆæ•¸å™¨å’Œä½ç½®ç‹€æ…‹")
                out_of_area = False
        
        # æ›´æ–°ä¸Šä¸€å¹€çš„çŸ©å½¢ä½ç½®
        last_rect_x = rect_center_x_in_full_frame

        if inside_area:
            if out_of_area and not rect_crossed_line:
                # åªæœ‰åœ¨éè·¨è¶Šä¸­é–“ç·šçš„æƒ…æ³ä¸‹ï¼Œé‡æ–°é€²å…¥æª¢æ¸¬å€æ‰é‡ç½®
                text_counter = Counter()
                text_conf_dict.clear()
                # é‡ç½®ç•¶å‰æœƒè©±çš„ä½ç½®æª¢æŸ¥ç‹€æ…‹
                current_session_position_checked = False
                current_session_position_status = "clear"
                current_session_warped_img = None
                #print("ğŸ“„ é‡æ–°é€²å…¥æª¢æ¸¬å€åŸŸ â†’ é‡ç½®è¨ˆæ•¸å™¨å’Œä½ç½®ç‹€æ…‹")
            out_of_area = False
            rect_crossed_line = False  # é‡ç½®è·¨è¶Šæ¨™è¨˜
        else:
            # ä»åœ¨ç•«é¢ä½†è¶…å‡º defect area ä¸é‡ç½®ï¼Œç­‰æ¶ˆå¤±å†è™•ç†
            pass

        warped = None
        #print(f"ğŸ” æª¢æ¸¬åˆ°çŸ©å½¢: x={x}, y={y}, w={w}, h={h}")
        #print(f"ğŸ“ right_halfå°ºå¯¸: {right_half.shape}")
        
        if best_approx is not None:
            #print(f"ğŸ”¶ ä½¿ç”¨é€è¦–è®Šæ›ï¼ŒçŸ©å½¢é»: {best_approx}")
            warped = four_point_transform(right_half, best_approx)
            if warped is None:
                #print("âš ï¸ é€è¦–è®Šæ›å¤±æ•—ï¼Œä½¿ç”¨é‚Šç•Œæ¡†è£åˆ‡")
                # ç¢ºä¿è£åˆ‡ç¯„åœåœ¨åœ–åƒé‚Šç•Œå…§
                y_start = max(0, y)
                y_end = min(right_half.shape[0], y + h)
                x_start = max(0, x)
                x_end = min(right_half.shape[1], x + w)
                #print(f"ğŸ“ é‚Šç•Œæ¡†è£åˆ‡ç¯„åœ: x={x_start}:{x_end}, y={y_start}:{y_end}")
                warped = right_half[y_start:y_end, x_start:x_end].copy()
            #else:
               # print(f"âœ… é€è¦–è®Šæ›æˆåŠŸï¼Œçµæœå°ºå¯¸: {warped.shape}")
        else:
            #print("ğŸ”³ ä½¿ç”¨é‚Šç•Œæ¡†è£åˆ‡")
            # ç¢ºä¿è£åˆ‡ç¯„åœåœ¨åœ–åƒé‚Šç•Œå…§
            y_start = max(0, y)
            y_end = min(right_half.shape[0], y + h)
            x_start = max(0, x)
            x_end = min(right_half.shape[1], x + w)
            #print(f"ğŸ“ é‚Šç•Œæ¡†è£åˆ‡ç¯„åœ: x={x_start}:{x_end}, y={y_start}:{y_end}")
            warped = right_half[y_start:y_end, x_start:x_end].copy()
        
        # æª¢æŸ¥warpedåœ–åƒæ˜¯å¦æœ‰æ•ˆ
        if warped is not None and warped.size > 0:
            print(f"âœ… Warpedåœ–åƒæœ‰æ•ˆï¼Œå°ºå¯¸: {warped.shape}")
        else:
            print("âŒ Warpedåœ–åƒç„¡æ•ˆï¼Œè·³éOCR")
            if warped is not None:
                print(f"   åœ–åƒå¤§å°: {warped.size}")
            warped = None

        # åªæœ‰åœ¨warpedåœ–åƒæœ‰æ•ˆæ™‚æ‰é€²è¡ŒOCRå’Œé¡¯ç¤º
        if warped is not None and frame_count % OCR_INTERVAL == 0:
           current_ocr_texts = run_ocr_and_collect_text(warped, conf_threshold)
        else:
            current_ocr_texts = {}
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œä½ç½®æª¢æ¸¬ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æª¢æ¸¬åˆ°æ–‡å­—ä¸”æœ‰æ¨™æº–ç­”æ¡ˆæ™‚é€²è¡Œï¼‰
        if (not current_session_position_checked and 
            current_ocr_texts and 
            reference_warped_img is not None and 
            reference_text is not None):
            
            # å–å¾—ç•¶å‰æª¢æ¸¬åˆ°çš„ç¬¬ä¸€å€‹æ–‡å­—é€²è¡Œä½ç½®æª¢æŸ¥
            first_detected_text = list(current_ocr_texts.keys())[0]
            #print(f"ğŸ¯ é¦–æ¬¡æª¢æ¸¬åˆ°æ–‡å­— '{first_detected_text}'ï¼Œé€²è¡Œä½ç½®æª¢æŸ¥...")
            
            iou_value, is_aligned = calculate_iou(
                warped, 
                first_detected_text, 
                threshold=0.6
            )
            
            current_session_position_status = "clear" if is_aligned else "misaligned"
            current_session_position_checked = True
            current_session_warped_img = warped.copy()  # ä¿å­˜é€™ä¸€å¹€çš„åœ–ç‰‡
            
            #print(f"âœ… ä½ç½®æª¢æŸ¥å®Œæˆ: IoU={iou_value:.3f}, Status={current_session_position_status}")
            #print("ğŸ’¡ æœ¬æ¬¡æœƒè©±ä¸å†é‡è¤‡é€²è¡Œä½ç½®æª¢æŸ¥")

        # é¡¯ç¤ºROIçª—å£ï¼ˆåªæœ‰åœ¨warpedæœ‰æ•ˆæ™‚ï¼‰
        if warped is not None and warped.size > 0:
            roi_display = warped.copy()
            if current_ocr_texts:
                # é¡¯ç¤ºæ¸…ç†å¾Œçš„æ–‡å­—ï¼ˆcurrent_ocr_texts.keys() å·²ç¶“æ˜¯æ¸…ç†éçš„ï¼‰
                cleaned_texts = list(current_ocr_texts.keys())
                text_line = " ".join(cleaned_texts)
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.8
                font_thickness = 2
                (text_width, text_height), baseline = cv2.getTextSize(text_line, font, font_scale, font_thickness)
                padding = 10
                cv2.rectangle(roi_display, (0, 0), (roi_display.shape[1], text_height + padding * 2), (255, 255, 255), -1)
                cv2.rectangle(roi_display, (0, 0), (roi_display.shape[1], text_height + padding * 2), (0, 0, 0), 2)
                cv2.putText(roi_display, text_line, (padding, text_height + padding),
                            font, font_scale, (0, 0, 0), font_thickness)
            
            # ä¿å­˜åŒ…å«OCRæ–‡æœ¬å åŠ çš„æ¨™æº–åŒ–ROIåœ–åƒ
            last_roi_image = roi_display.copy()
            cv2.imshow("ROI + OCR Compare", roi_display)

        if best_approx is not None:
            pts_int = best_approx.astype(int)
            pts_int[:, 0] += line_x  # èª¿æ•´åº§æ¨™åç§»
            cv2.polylines(frame, [pts_int], True, (255, 0, 0), 2)
        else:
            cv2.rectangle(frame, (x + line_x, y), (x + line_x + w, y + h), (255, 0, 0), 2)

        top3 = text_counter.most_common(3)
        y0 = 30
        for i, (word, count) in enumerate(top3):
            y = y0 + i * 30
            cv2.putText(frame, f"{word}: {count}", (10, y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    else:
        # æ²’æœ‰æ‰¾åˆ°çŸ©å½¢ â†’ è¦–ç‚ºé›¢é–‹
        last_rect_x = None  # é‡ç½®ä½ç½®è¿½è¹¤
        
        if not out_of_area:
            # åœ¨é›¢é–‹å‰è¼¸å‡º JSON
            if text_counter:
                top1, count = text_counter.most_common(1)[0]
                confidence = text_conf_dict.get(top1, 0.0)
                
                # ä½¿ç”¨å·²ç¶“æª¢æŸ¥éçš„ä½ç½®ç‹€æ…‹
                position_status = current_session_position_status
                #print(f"ğŸ“‹ ä½¿ç”¨å·²è¨˜éŒ„çš„ä½ç½®ç‹€æ…‹: {position_status}")
                
                # å°‡æœ€å¾Œçš„ROIåœ–åƒè½‰æ›ç‚ºbase64ï¼ˆå„ªå…ˆä½¿ç”¨ä½ç½®æª¢æŸ¥æ™‚ä¿å­˜çš„åœ–ç‰‡ï¼‰
                image_base64 = ""
                roi_image_to_save = current_session_warped_img if current_session_warped_img is not None else last_roi_image
                if roi_image_to_save is not None:
                    image_base64 = image_to_base64(roi_image_to_save)

                result_json = {
                    serial_number: {
                        "value": top1,
                        "confidence": round(confidence, 3),
                        "status": position_status,
                        #"bbox": current_bbox,  # æ·»åŠ åº§æ¨™è³‡è¨Š
                        "image_base64": "none" #image_base64
                    }
                }
                print("ğŸ“¤ JSON result:", result_json)
                serial_number += 1

            #print("â¬…ï¸ Lost detection â†’ mark as out of area")
            out_of_area = True

    # ç•«æª¢æ¸¬å€æ¡†ç·š
    cv2.line(frame, (line_x, 0), (line_x, height), (0, 255, 255), 2)

    cv2.putText(frame, f"Frame: {frame_count}", (10, 120),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, 150),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

    cv2.imshow("Frame", frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break
    elif key == ord("a"):  # å‘å·¦ç§»å‹•ç·š
        LINE_POSITION_RATIO = max(0.1, LINE_POSITION_RATIO - 0.05)
        print(f"ğŸ“ ç·šä½ç½®èª¿æ•´ç‚º: {LINE_POSITION_RATIO:.2f} (å‘å·¦)")
    elif key == ord("d"):  # å‘å³ç§»å‹•ç·š
        LINE_POSITION_RATIO = min(0.9, LINE_POSITION_RATIO + 0.05)
        print(f"ğŸ“ ç·šä½ç½®èª¿æ•´ç‚º: {LINE_POSITION_RATIO:.2f} (å‘å³)")

cap.release()
cv2.destroyAllWindows()
